{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcUXmaMSqVAUL9YlMAD/DP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhaveshNikam09/Deep_learning_practise/blob/main/dog_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baia1F9HBWzk",
        "outputId": "e30323d4-e2ea-4fa4-b83f-58583e328ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/dog-and-cat-classification-dataset\n",
            "['PetImages']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_path = \"/kaggle/input/dog-and-cat-classification-dataset/PetImages\"\n",
        "\n",
        "categories=['Dog','Cat']\n",
        "\n",
        "image_path=[]\n",
        "labels=[]\n",
        "\n",
        "for category in categories:\n",
        "  class_path=os.path.join(dataset_path,category)\n",
        "  images=os.listdir(class_path)\n",
        "\n",
        "  for image in images:\n",
        "    image_path.append(os.path.join(class_path,image))\n",
        "    labels.append(0 if category == \"Cat\" else 1)\n",
        "\n",
        "labels=np.array(labels)\n",
        "\n",
        "train_path,test_path,train_labels,test_labels=train_test_split(image_path,labels,test_size=0.2,random_state=42)\n",
        "\n",
        "print(f\"Total images: {len(image_path)}\")\n",
        "print(f\"Train images: {len(train_path)}, Test images: {len(test_path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMmZrBQjB3x1",
        "outputId": "09a02b83-402a-481f-fd22-90cc7f33cd51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 24998\n",
            "Train images: 19998, Test images: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to load and normalize images in small batches\n",
        "def load_and_normalize_images(image_paths, target_size=(224, 224), batch_size=1000):\n",
        "    images = []\n",
        "    for i, path in enumerate(image_paths):\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            print(f\"⚠️ Skipping corrupt/missing image: {path}\")\n",
        "            continue\n",
        "        img = cv2.resize(img, target_size)\n",
        "        img = img / 255.0\n",
        "        images.append(img)\n",
        "\n",
        "        # Process in small batches to avoid crashes\n",
        "        if (i + 1) % batch_size == 0:\n",
        "            print(f\"✅ Processed {i+1}/{len(image_paths)} images...\")\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "# Load and normalize train & test images in batches\n",
        "X_train = load_and_normalize_images(train_path, batch_size=500)\n",
        "X_test = load_and_normalize_images(test_path, batch_size=500)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APQLJvLeCGoX",
        "outputId": "69de03f9-a3dd-4f1a-aacd-1504404aa95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/9171.jpg\n",
            "✅ Processed 500/19998 images...\n",
            "✅ Processed 1000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/8470.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/10158.jpg\n",
            "✅ Processed 1500/19998 images...\n",
            "✅ Processed 2000/19998 images...\n",
            "✅ Processed 2500/19998 images...\n",
            "✅ Processed 3000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/7969.jpg\n",
            "✅ Processed 3500/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/3288.jpg\n",
            "✅ Processed 4000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/850.jpg\n",
            "✅ Processed 4500/19998 images...\n",
            "✅ Processed 5000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/660.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/140.jpg\n",
            "✅ Processed 5500/19998 images...\n",
            "✅ Processed 6000/19998 images...\n",
            "✅ Processed 6500/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/1308.jpg\n",
            "✅ Processed 7000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/7968.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/4833.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat/9778.jpg\n",
            "✅ Processed 7500/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/10797.jpg\n",
            "✅ Processed 8000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/11675.jpg\n",
            "✅ Processed 8500/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/6238.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/7369.jpg\n",
            "✅ Processed 9000/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/8730.jpg\n",
            "✅ Processed 9500/19998 images...\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/6059.jpg\n",
            "⚠️ Skipping corrupt/missing image: /kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog/11410.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame for training data\n",
        "train_df = pd.DataFrame({\"filename\": train_path, \"label\": train_labels})\n",
        "test_df = pd.DataFrame({\"filename\": test_path, \"label\": test_labels})\n",
        "\n",
        "# Check the structure\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uURi3qEmNYpp",
        "outputId": "997203d5-d73d-4eaf-e95a-cdf54db9f573"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            filename  label\n",
            "0  /kaggle/input/dog-and-cat-classification-datas...      0\n",
            "1  /kaggle/input/dog-and-cat-classification-datas...      0\n",
            "2  /kaggle/input/dog-and-cat-classification-datas...      1\n",
            "3  /kaggle/input/dog-and-cat-classification-datas...      0\n",
            "4  /kaggle/input/dog-and-cat-classification-datas...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numeric labels to string labels\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
        "test_df[\"label\"] = test_df[\"label\"].astype(str)\n"
      ],
      "metadata": {
        "id": "vb1slzMZb95t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def check_and_fix_images(image_paths):\n",
        "    problematic_images = []\n",
        "\n",
        "    for file_path in image_paths:\n",
        "        try:\n",
        "            # Open image with PIL\n",
        "            with Image.open(file_path) as img:\n",
        "                img = img.convert(\"RGB\")  # Ensure RGB format\n",
        "                img_array = np.array(img)  # Convert to NumPy array\n",
        "\n",
        "                # Check if image array is empty (corrupt image)\n",
        "                if img_array is None or img_array.size == 0:\n",
        "                    print(f\"Empty image detected: {file_path}\")\n",
        "                    problematic_images.append(file_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Problematic image found: {file_path} - {e}\")\n",
        "            problematic_images.append(file_path)\n",
        "\n",
        "    return problematic_images\n",
        "\n",
        "# Run the function again\n",
        "problematic_files = check_and_fix_images(train_path)\n",
        "print(f\"Total problematic images: {len(problematic_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUf2gDwDh2yZ",
        "outputId": "f4efe382-7757-4a2b-bbe0-d5001e83533a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total problematic images: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in problematic_files:\n",
        "    os.remove(file)\n",
        "\n",
        "print(\"Corrupt images removed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFS_CS5oiDJK",
        "outputId": "2064ca14-7084-4578-a07b-a12f7439162a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupt images removed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with image paths and labels\n",
        "train_df = pd.DataFrame({'filename': train_path, 'label': train_labels})\n",
        "test_df = pd.DataFrame({'filename': test_path, 'label': test_labels})\n",
        "\n",
        "# Convert numerical labels to strings (required by flow_from_dataframe)\n",
        "train_df['label'] = train_df['label'].astype(str)\n",
        "test_df['label'] = test_df['label'].astype(str)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col=\"filename\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col=\"filename\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l-YhVxfb2QK",
        "outputId": "ea0ad06f-0fa2-4886-ef3a-445b6c52c40a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19998 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "783oMTyoiCEP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "eH09BLBMcIAv",
        "outputId": "56cb3ad4-dfb8-4c36-b1cb-afb45dca93eb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,788,673\u001b[0m (56.41 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,788,673</span> (56.41 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m73,985\u001b[0m (289.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,985</span> (289.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_generator,epochs=50,validation_data=test_generator,batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBAZXYeddvO5",
        "outputId": "f42af9dd-894f-4010-919f-ac9985f913fb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 58/625\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 175ms/step - accuracy: 0.6413 - loss: 0.6210"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 256ms/step - accuracy: 0.8298 - loss: 0.3589 - val_accuracy: 0.9238 - val_loss: 0.1905\n",
            "Epoch 2/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 220ms/step - accuracy: 0.9210 - loss: 0.1908 - val_accuracy: 0.9116 - val_loss: 0.2190\n",
            "Epoch 3/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9218 - loss: 0.1847 - val_accuracy: 0.9272 - val_loss: 0.1771\n",
            "Epoch 4/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9267 - loss: 0.1696 - val_accuracy: 0.9130 - val_loss: 0.2143\n",
            "Epoch 5/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9340 - loss: 0.1586 - val_accuracy: 0.9252 - val_loss: 0.1775\n",
            "Epoch 6/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9331 - loss: 0.1573 - val_accuracy: 0.9258 - val_loss: 0.1844\n",
            "Epoch 7/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9355 - loss: 0.1549 - val_accuracy: 0.9174 - val_loss: 0.2170\n",
            "Epoch 8/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9378 - loss: 0.1470 - val_accuracy: 0.9214 - val_loss: 0.1909\n",
            "Epoch 9/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9377 - loss: 0.1461 - val_accuracy: 0.9298 - val_loss: 0.1707\n",
            "Epoch 10/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9452 - loss: 0.1329 - val_accuracy: 0.9226 - val_loss: 0.1850\n",
            "Epoch 11/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 241ms/step - accuracy: 0.9416 - loss: 0.1387 - val_accuracy: 0.9320 - val_loss: 0.1752\n",
            "Epoch 12/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9492 - loss: 0.1282 - val_accuracy: 0.9266 - val_loss: 0.1785\n",
            "Epoch 13/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9503 - loss: 0.1233 - val_accuracy: 0.9254 - val_loss: 0.1903\n",
            "Epoch 14/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9491 - loss: 0.1249 - val_accuracy: 0.9262 - val_loss: 0.1853\n",
            "Epoch 15/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 220ms/step - accuracy: 0.9502 - loss: 0.1215 - val_accuracy: 0.9310 - val_loss: 0.1793\n",
            "Epoch 16/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 220ms/step - accuracy: 0.9473 - loss: 0.1256 - val_accuracy: 0.9330 - val_loss: 0.1782\n",
            "Epoch 17/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9526 - loss: 0.1135 - val_accuracy: 0.9134 - val_loss: 0.2084\n",
            "Epoch 18/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9553 - loss: 0.1107 - val_accuracy: 0.9362 - val_loss: 0.1763\n",
            "Epoch 19/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 220ms/step - accuracy: 0.9560 - loss: 0.1074 - val_accuracy: 0.9344 - val_loss: 0.1838\n",
            "Epoch 20/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9584 - loss: 0.1000 - val_accuracy: 0.9324 - val_loss: 0.1874\n",
            "Epoch 21/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9581 - loss: 0.1033 - val_accuracy: 0.9354 - val_loss: 0.1836\n",
            "Epoch 22/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9625 - loss: 0.0912 - val_accuracy: 0.9266 - val_loss: 0.2080\n",
            "Epoch 23/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 220ms/step - accuracy: 0.9615 - loss: 0.0931 - val_accuracy: 0.9292 - val_loss: 0.1961\n",
            "Epoch 24/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9658 - loss: 0.0885 - val_accuracy: 0.9314 - val_loss: 0.1954\n",
            "Epoch 25/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 242ms/step - accuracy: 0.9656 - loss: 0.0858 - val_accuracy: 0.9232 - val_loss: 0.2118\n",
            "Epoch 26/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9669 - loss: 0.0848 - val_accuracy: 0.9342 - val_loss: 0.1941\n",
            "Epoch 27/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9660 - loss: 0.0861 - val_accuracy: 0.9346 - val_loss: 0.2171\n",
            "Epoch 28/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 242ms/step - accuracy: 0.9697 - loss: 0.0782 - val_accuracy: 0.9312 - val_loss: 0.2373\n",
            "Epoch 29/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 222ms/step - accuracy: 0.9715 - loss: 0.0761 - val_accuracy: 0.9262 - val_loss: 0.2454\n",
            "Epoch 30/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 241ms/step - accuracy: 0.9725 - loss: 0.0730 - val_accuracy: 0.9260 - val_loss: 0.2362\n",
            "Epoch 31/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9723 - loss: 0.0725 - val_accuracy: 0.9244 - val_loss: 0.2789\n",
            "Epoch 32/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9711 - loss: 0.0712 - val_accuracy: 0.9298 - val_loss: 0.2480\n",
            "Epoch 33/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 242ms/step - accuracy: 0.9766 - loss: 0.0633 - val_accuracy: 0.9304 - val_loss: 0.2587\n",
            "Epoch 34/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9757 - loss: 0.0610 - val_accuracy: 0.9290 - val_loss: 0.2524\n",
            "Epoch 35/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 220ms/step - accuracy: 0.9781 - loss: 0.0561 - val_accuracy: 0.9266 - val_loss: 0.2792\n",
            "Epoch 36/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9749 - loss: 0.0614 - val_accuracy: 0.9288 - val_loss: 0.2532\n",
            "Epoch 37/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9792 - loss: 0.0559 - val_accuracy: 0.9300 - val_loss: 0.2780\n",
            "Epoch 38/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9796 - loss: 0.0505 - val_accuracy: 0.9334 - val_loss: 0.2483\n",
            "Epoch 39/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9831 - loss: 0.0445 - val_accuracy: 0.9310 - val_loss: 0.2820\n",
            "Epoch 40/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 242ms/step - accuracy: 0.9795 - loss: 0.0532 - val_accuracy: 0.9114 - val_loss: 0.3260\n",
            "Epoch 41/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 222ms/step - accuracy: 0.9813 - loss: 0.0513 - val_accuracy: 0.9266 - val_loss: 0.2800\n",
            "Epoch 42/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 241ms/step - accuracy: 0.9824 - loss: 0.0468 - val_accuracy: 0.9320 - val_loss: 0.2915\n",
            "Epoch 43/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9834 - loss: 0.0422 - val_accuracy: 0.9282 - val_loss: 0.3018\n",
            "Epoch 44/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9848 - loss: 0.0401 - val_accuracy: 0.9240 - val_loss: 0.2914\n",
            "Epoch 45/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9820 - loss: 0.0459 - val_accuracy: 0.9266 - val_loss: 0.3365\n",
            "Epoch 46/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9866 - loss: 0.0359 - val_accuracy: 0.9240 - val_loss: 0.3425\n",
            "Epoch 47/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9854 - loss: 0.0391 - val_accuracy: 0.9248 - val_loss: 0.3126\n",
            "Epoch 48/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9873 - loss: 0.0371 - val_accuracy: 0.9216 - val_loss: 0.3209\n",
            "Epoch 49/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9861 - loss: 0.0374 - val_accuracy: 0.9234 - val_loss: 0.3562\n",
            "Epoch 50/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 221ms/step - accuracy: 0.9858 - loss: 0.0347 - val_accuracy: 0.9284 - val_loss: 0.3320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=cv2.imread(\"/content/download.jpeg\")\n",
        "img=cv2.resize(img,(224,224))\n",
        "img=img/255.0\n",
        "img=np.expand_dims(img,axis=0)\n",
        "prediction=model.predict(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emDDFOMAgBsy",
        "outputId": "960c4c7e-fb5f-4d21-eeba-e9b43056b4ee"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = 1 if prediction >= 0.5 else 0  # Convert probability to class\n",
        "\n",
        "if label==1:\n",
        "  print(\"Dog\")\n",
        "else:\n",
        "  print(\"Cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ2X4aGRgXlV",
        "outputId": "7ced76e9-71c4-45e4-ec19-9fd2c9ba025b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n",
            "Cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=cv2.imread(\"/content/images.jpeg\")\n",
        "img=cv2.resize(img,(224,224))\n",
        "img=img/255.0\n",
        "img=np.expand_dims(img,axis=0)\n",
        "prediction=model.predict(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCo22kF_D-I",
        "outputId": "e96efaa3-05c5-4160-d3f3-3403b6e5bb07"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = 1 if prediction >= 0.5 else 0  # Convert probability to class\n",
        "\n",
        "if label==1:\n",
        "  print(\"Dog\")\n",
        "else:\n",
        "  print(\"Cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLRJmEhY_nPU",
        "outputId": "37a90d9b-8fee-4b59-cf33-4024a84b1582"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=cv2.imread(\"/content/download (1).jpeg\")\n",
        "img=cv2.resize(img,(224,224))\n",
        "img=img/255.0\n",
        "img=np.expand_dims(img,axis=0)\n",
        "prediction=model.predict(img)\n",
        "label = 1 if prediction >= 0.5 else 0  # Convert probability to class\n",
        "\n",
        "if label==1:\n",
        "  print(\"Dog\")\n",
        "else:\n",
        "  print(\"Cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnMck9_s_qSG",
        "outputId": "410df623-f2a2-4f15-8cf1-09f31c2e3003"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imread('/content/download (1).jpeg')\n",
        "prediction=model.predict(img)\n",
        "label = 1 if prediction >= 0.5 else 0  # Convert probability to class\n",
        "\n",
        "if label==1:\n",
        "  print(\"Dog\")\n",
        "else:\n",
        "  print(\"Cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269fYAsv_9DY",
        "outputId": "1d046cf0-f910-4665-c403-9524649443f2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQpg3L77ADFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}